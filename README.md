# Ollama CLI Assistant with MCP Support

A powerful command-line interface that integrates Ollama with Model Context Protocol (MCP) for enhanced AI assistance with tool capabilities.

## ğŸŒŸ Features

- **Interactive Chat Mode** - Natural conversation with your local LLM
- **MCP Integration** - Connect to multiple MCP servers for extended capabilities
- **File Operations** - Read and discuss files with AI assistance
- **Shell Execution** - Execute commands with AI explanations
- **Multiple Model Support** - Easy switching between Ollama models
- **Conversation History** - Save and restore chat sessions
- **Streaming Responses** - Real-time AI output
- **System Prompts** - Customize AI behavior
- **Persistent Configuration** - Settings saved between sessions

## ğŸ“‹ Prerequisites

- Python 3.8+
- [Ollama](https://ollama.ai) installed and running
- Node.js (for MCP servers)

## ğŸš€ Installation

### 1. Clone the repository

```bash
git clone https://github.com/larrybuckalew/ollama-cli-assistant.git
cd ollama-cli-assistant
```

### 2. Install Python dependencies

```bash
pip install -r requirements.txt
```

### 3. Install MCP servers (optional but recommended)

```bash
# Filesystem access
npm install -g @modelcontextprotocol/server-filesystem

# GitHub integration
npm install -g @modelcontextprotocol/server-github

# Web search
npm install -g @modelcontextprotocol/server-brave-search

# PostgreSQL database
npm install -g @modelcontextprotocol/server-postgres
```

### 4. Make the script executable (Linux/Mac)

```bash
chmod +x ollama-cli.py
```

### 5. Optional: Add to PATH

```bash
# Add to ~/.bashrc or ~/.zshrc
echo 'alias ai="python3 /path/to/ollama-cli.py"' >> ~/.bashrc
source ~/.bashrc
```

## ğŸ¯ Quick Start

### Basic Usage

```bash
# Interactive mode
python ollama-cli.py

# Single query
python ollama-cli.py -q "What is Python?"

# Use specific model
python ollama-cli.py -m codellama

# List available models
python ollama-cli.py --list-models
```

### First Time Setup

1. Start Ollama: `ollama serve`
2. Pull a model: `ollama pull llama2`
3. Run the CLI: `python ollama-cli.py`
4. Enable MCP servers: Type `/mcp` in the chat

## ğŸ“– Commands

### Chat Commands

| Command | Description |
|---------|-------------|
| `/help` | Show available commands |
| `/clear` | Clear conversation history |
| `/models` | List available Ollama models |
| `/switch` | Switch to a different model |
| `/info` | Show system information |
| `/read <file>` | Read and discuss a file |
| `/exec <cmd>` | Execute shell command |
| `/save` | Save conversation history |
| `/config` | Show current configuration |
| `/system` | Set custom system prompt |
| `/mcp` | Manage MCP servers |
| `/tools` | List available MCP tools |
| `/exit` | Exit the assistant |

### Command Line Arguments

```bash
python ollama-cli.py [OPTIONS]

Options:
  -m, --model MODEL        Specify Ollama model (default: llama2)
  -q, --query QUERY        Single query mode
  --url URL                Ollama API URL (default: http://localhost:11434)
  --list-models            List available models
  --reset-config           Reset configuration to defaults
  --no-mcp                 Disable MCP integration
  -h, --help               Show help message
```

## ğŸ”§ MCP Configuration

### Enable MCP Servers

1. In the chat, type `/mcp`
2. Choose option `e` to enable a server
3. Enter the server name (e.g., `filesystem`)
4. Choose option `r` to reconnect

### Configure Server Settings

Edit `~/.ollama-cli/mcp_servers.json` (or `%USERPROFILE%\.ollama-cli\mcp_servers.json` on Windows):

```json
{
  "filesystem": {
    "command": "npx",
    "args": ["-y", "@modelcontextprotocol/server-filesystem", "/home/user"],
    "enabled": true
  },
  "github": {
    "command": "npx",
    "args": ["-y", "@modelcontextprotocol/server-github"],
    "env": {
      "GITHUB_PERSONAL_ACCESS_TOKEN": "ghp_your_token_here"
    },
    "enabled": true
  }
}
```

### Available MCP Servers

- **Filesystem** - File read/write, directory listing
- **GitHub** - Repository access, issues, pull requests
- **Brave Search** - Web search integration
- **PostgreSQL** - Database queries and operations

## ğŸ’¡ Use Cases

### Code Review
```
You: Read src/main.py and suggest improvements
```

### System Debugging
```
You: /exec docker ps
You: What does this output mean?
```

### Research
```
You: Search for latest developments in AI safety
```

### Database Operations
```
You: Query the users table where created_at > '2024-01-01'
```

## ğŸ“ Project Structure

```
ollama-cli-assistant/
â”œâ”€â”€ ollama-cli.py          # Main application
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ README.md             # This file
â”œâ”€â”€ LICENSE               # MIT License
â””â”€â”€ .gitignore            # Git ignore rules
```

## ğŸ” Security

- MCP tools run with your system permissions
- API keys are stored locally in `~/.ollama-cli/mcp_servers.json`
- Shell commands require confirmation before execution
- File access is limited to configured directories

**Important:** Keep your configuration files secure and never commit API keys to version control.

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- [Ollama](https://ollama.ai) - Local LLM runtime
- [Model Context Protocol](https://modelcontextprotocol.io) - Tool integration framework
- [Anthropic](https://anthropic.com) - For MCP specification

## ğŸ“® Support

- **Issues:** [GitHub Issues](https://github.com/larrybuckalew/ollama-cli-assistant/issues)
- **Discussions:** [GitHub Discussions](https://github.com/larrybuckalew/ollama-cli-assistant/discussions)

## ğŸ—ºï¸ Roadmap

- [ ] Plugin system for custom commands
- [ ] Multi-language support
- [ ] Voice input/output
- [ ] Web interface
- [ ] Docker container
- [ ] More MCP server integrations

---

**Author:** Larry Buckalew

Made with â¤ï¸ for the AI community
